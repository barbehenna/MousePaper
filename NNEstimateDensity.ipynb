{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mouse Population Density Estimation\n",
    "\n",
    "Here I'm playing around with creating a small neural network to take in the results of the simualtions and try to predict the true population density.\n",
    "\n",
    "## Notes\n",
    "\n",
    "I've tried using just the density estimations of each square and it didn't do much better than a linear model (somewhat unsurprisingly). I'm **now incorperating trap spacing and catch radius**. \n",
    "\n",
    "## To-Do\n",
    "\n",
    "- Use some sort of inverse weighting scheme because most of the estimates are already good because the model works well. We need to emphasize the areas where the method doesn't work as well to beat a linear model.\n",
    "- Try adding in other metrics (variance of the error, for instance) and maybe even make it the target for training rather than L2 error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import mxnet as mx\n",
    "from mxnet import nd, autograd, gluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ctx = mx.gpu()\n",
    "model_ctx = mx.gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simdata = pd.read_csv(\"data/trainingdata.csv\")\n",
    "simdata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100000\n",
    "\n",
    "# num_train_samples = int(simdata.shape[0]*.08)\n",
    "# num_test_samples = int(simdata.shape[0]*.01)\n",
    "num_train_samples = 100*batch_size\n",
    "num_test_samples = 25*batch_size\n",
    "\n",
    "print(\"Num batches: train =\", num_train_samples/batch_size, \"test =\", num_test_samples/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simtrain = simdata.sample(n=num_train_samples)\n",
    "simtest = simdata.drop(simtrain.index).sample(num_test_samples)\n",
    "\n",
    "predictors = [\"square1\", \"square2\",\"square3\", \"square4\",\"square5\", \"square6\",\"square7\", \"square8\", \"TrapSpacing\", \"CatchRadius\"]\n",
    "\n",
    "Xtrain = nd.array(simtrain[predictors], ctx=data_ctx)\n",
    "Ytrain = nd.array(simtrain[\"Density\"], ctx=data_ctx)\n",
    "\n",
    "Xtest = nd.array(simtest[predictors], ctx=data_ctx)\n",
    "Ytest = nd.array(simtest[\"Density\"], ctx=data_ctx)\n",
    "\n",
    "train_data = gluon.data.DataLoader(gluon.data.ArrayDataset(Xtrain, Ytrain), batch_size=batch_size, shuffle=True)\n",
    "test_data = gluon.data.DataLoader(gluon.data.ArrayDataset(Xtest, Ytest), batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = gluon.nn.Sequential()\n",
    "\n",
    "with net.name_scope():\n",
    "    net.add(gluon.nn.Dense(8, activation=\"relu\"))\n",
    "    net.add(gluon.nn.Dense(8, activation=\"relu\"))\n",
    "    net.add(gluon.nn.Dense(8, activation=\"relu\"))\n",
    "    net.add(gluon.nn.Dense(1))\n",
    "    net.collect_params().initialize(mx.init.Normal(sigma=1.), ctx=model_ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "square_loss = gluon.loss.L2Loss()\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': 0.00001})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_train_sequence = []\n",
    "loss_test_sequence = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "\n",
    "for e in range(epochs):\n",
    "    # train epoch loop\n",
    "    cumulative_train_loss = 0\n",
    "    for i, (data, label) in enumerate(train_data):\n",
    "        data = data.as_in_context(model_ctx)\n",
    "        label = label.as_in_context(model_ctx)\n",
    "        with autograd.record():\n",
    "            output = net(data)\n",
    "            loss = square_loss(output, label)\n",
    "        loss.backward()\n",
    "        trainer.step(batch_size)\n",
    "        cumulative_train_loss += nd.sum(loss).asscalar()\n",
    "    loss_train_sequence.append(cumulative_train_loss/num_train_samples)\n",
    "    \n",
    "#     print(\"Epoch %s, train loss: %s\" % (e, cumulative_train_loss/num_train_samples))\n",
    "\n",
    "    # test epoch loss loop\n",
    "    cumulative_test_loss = 0\n",
    "    for i, (data, label) in enumerate(test_data):\n",
    "        data = data.as_in_context(model_ctx)\n",
    "        label = label.as_in_context(model_ctx)\n",
    "        output = net(data)\n",
    "        loss = square_loss(output, label)\n",
    "        cumulative_test_loss += nd.sum(loss).asscalar()\n",
    "    loss_test_sequence.append(cumulative_test_loss/num_test_samples)\n",
    "    \n",
    "    print(\"Epoch %s, train loss: %s, test loss: %s\" % (e, cumulative_train_loss/num_train_samples, cumulative_test_loss/num_test_samples))\n",
    "    if e % 10 is 0:\n",
    "        print(\"Test values:\", Ytest[0:5].asnumpy())\n",
    "        print(\"Predicted values:\", net(Xtest)[0:5,0].asnumpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the convergence of the estimated loss function\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(num=None,figsize=(8, 6))\n",
    "plt.plot(loss_train_sequence[10:])\n",
    "plt.plot(loss_test_sequence[10:])\n",
    "\n",
    "# Adding some bells and whistles to the plot\n",
    "plt.grid(True, which=\"both\")\n",
    "plt.xlabel('epoch',fontsize=14)\n",
    "plt.ylabel('average loss',fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.summary(nd.random.uniform(shape=(8,10), ctx=data_ctx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "square_loss(net(Xtrain), Ytrain).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "square_loss(net(Xtest), Ytest).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist((Ytest.flatten()-net(Xtest)).asnumpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
