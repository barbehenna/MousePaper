{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating Density from Raw Trap Data\n",
    "\n",
    "I'm taking in the trap counts per day across the trap grid as a 4 channel 16x16 image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "from mxnet import nd, gluon, autograd\n",
    "from mxboard import SummaryWriter\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "batch_size = 100000\n",
    "epochs = 1000\n",
    "learning_rate = 0.01\n",
    "trainperc = 0.8\n",
    "\n",
    "context = mx.gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "simimages = torch.load(\"data/simimages.pt\")\n",
    "simimages = nd.array(simimages)\n",
    "\n",
    "simdensities = torch.load(\"data/simdensities.pt\")\n",
    "simdensities = nd.array(simdensities)\n",
    "\n",
    "numsamples = simdensities.shape[0]\n",
    "num_train_samples = round(trainperc*numsamples)\n",
    "num_test_samples = numsamples - round(trainperc*numsamples)\n",
    "\n",
    "numbatches = num_train_samples / batch_size\n",
    "numbatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = gluon.data.DataLoader(gluon.data.ArrayDataset(simimages[:num_train_samples,:,:,:], \n",
    "                                                           simdensities[:num_train_samples]), \n",
    "                                   batch_size=batch_size, shuffle=True)\n",
    "test_data = gluon.data.DataLoader(gluon.data.ArrayDataset(simimages[num_train_samples:,:,:,:], \n",
    "                                                          simdensities[num_train_samples:]), \n",
    "                                  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reshape2D1D(gluon.Block):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Reshape2D1D, self).__init__(**kwargs)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x.reshape((x.shape[0],x.shape[1],256))\n",
    "\n",
    "class Reshape1D2D(gluon.Block):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Reshape1D2D, self).__init__(**kwargs)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x.reshape((x.shape[0],x.shape[1],8,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test layers (they reverse each other :) one example here)\n",
    "# net = Reshape2D1D()\n",
    "# net(x).shape\n",
    "\n",
    "# oneway = gluon.nn.Sequential()\n",
    "# oneway.add(Reshape2D1D())\n",
    "# oneway.add(Reshape1D2D())\n",
    "\n",
    "# (oneway(x) == x).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "# net = gluon.nn.Sequential()\n",
    "\n",
    "# with net.name_scope():\n",
    "#     net.add(gluon.nn.Conv2D(channels=5, kernel_size=3, activation='relu'))\n",
    "#     net.add(gluon.nn.Conv2D(channels=5, kernel_size=3, activation='relu'))\n",
    "#     net.add(gluon.nn.MaxPool2D(pool_size=2, strides=2))\n",
    "#     net.add(gluon.nn.Conv2D(channels=5, kernel_size=3, activation='relu'))\n",
    "#     net.add(gluon.nn.MaxPool2D(pool_size=2, strides=2))\n",
    "#     # The Flatten layer collapses all axis, except the first one, into one axis.\n",
    "#     net.add(gluon.nn.Flatten())\n",
    "#     net.add(gluon.nn.Dense(8, activation='relu'))\n",
    "#     net.add(gluon.nn.Dense(8, activation='relu'))\n",
    "#     net.add(gluon.nn.Dense(1))\n",
    "\n",
    "# net\n",
    "\n",
    "net = gluon.nn.Sequential()\n",
    "\n",
    "with net.name_scope():\n",
    "    net.add(Reshape2D1D())\n",
    "    net.add(gluon.nn.Conv1D(channels=8, kernel_size=1, activation='relu'))\n",
    "    net.add(gluon.nn.Conv1D(channels=8, kernel_size=1, activation='relu'))\n",
    "    net.add(gluon.nn.MaxPool1D(pool_size=2))\n",
    "    \n",
    "    net.add(gluon.nn.Conv1D(channels=8, kernel_size=1, activation='relu'))\n",
    "    net.add(gluon.nn.Conv1D(channels=8, kernel_size=1, activation='relu'))\n",
    "    net.add(gluon.nn.MaxPool1D(pool_size=2))\n",
    "    \n",
    "    net.add(Reshape1D2D())\n",
    "    net.add(gluon.nn.Conv2D(channels=4, kernel_size=3, activation='relu'))\n",
    "    net.add(gluon.nn.Conv2D(channels=4, kernel_size=3, activation='relu'))\n",
    "    net.add(gluon.nn.MaxPool2D(pool_size=2, strides=2))\n",
    "    \n",
    "    # The Flatten layer collapses all axis, except the first one, into one axis.\n",
    "    net.add(gluon.nn.Flatten())\n",
    "    net.add(gluon.nn.Dense(8, activation='relu'))\n",
    "    net.add(gluon.nn.Dense(8, activation='relu'))\n",
    "    net.add(gluon.nn.Dense(8, activation='relu'))\n",
    "    net.add(gluon.nn.Dense(1))\n",
    "\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter initialization\n",
    "net.collect_params().initialize(mx.init.Normal(sigma=1.), ctx=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.summary(nd.random.uniform(shape=(batch_size, 4, 16, 16), ctx=context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer and metrics\n",
    "trainer = gluon.Trainer(net.collect_params(), 'adam', {'learning_rate': learning_rate})\n",
    "\n",
    "metric_MSE = mx.metric.MSE() # train metric\n",
    "loss_L2 = gluon.loss.L2Loss() # L2 loss\n",
    "\n",
    "# define loop to test the model\n",
    "def test(ctx):\n",
    "    metric = mx.metric.MSE()\n",
    "    for data, label in test_data:\n",
    "        data = data.as_in_context(ctx)\n",
    "        label = label.as_in_context(ctx)\n",
    "        output = net(data)\n",
    "        metric.update([label], [output])\n",
    "\n",
    "    return metric.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a summary writer that logs data and flushes to the file every 5 seconds\n",
    "sw = SummaryWriter(logdir='./logs', flush_secs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "global_step = 0\n",
    "for e in range(epochs):\n",
    "#     e += 1000\n",
    "    # reset data iterator and metric at begining of epoch.\n",
    "    metric_MSE.reset()\n",
    "    \n",
    "    # Loop through the training data\n",
    "    for i, (data, label) in enumerate(train_data):\n",
    "        # Copy data to context (ctx) if necessary\n",
    "        data = data.as_in_context(context)\n",
    "        label = label.as_in_context(context)\n",
    "        # Start recording computation graph with record() section.\n",
    "        # Recorded graphs can then be differentiated with backward.\n",
    "        with autograd.record():\n",
    "            output = net(data)\n",
    "            L = loss_L2(output, label)\n",
    "        \n",
    "        sw.add_scalar(tag='train_loss', value=L.mean().asscalar(), global_step=global_step)\n",
    "        global_step += 1\n",
    "        L.backward()\n",
    "        \n",
    "        # take a gradient step with batch_size equal to data.shape[0]\n",
    "        trainer.step(data.shape[0])\n",
    "        \n",
    "        # update metric at last.\n",
    "        metric_MSE.update([label], [output])\n",
    "        \n",
    "    # logging training accuracy\n",
    "    name, train_acc = metric_MSE.get()\n",
    "    sw.add_scalar(tag='accuracy_curves', value=('train_acc', train_acc), global_step=e)\n",
    "    \n",
    "    # logging testing accuracy\n",
    "    name, test_acc = test(context)\n",
    "    sw.add_scalar(tag='accuracy_curves', value=('valid_acc', test_acc), global_step=e)\n",
    "    \n",
    "#     print(\"epoch:\", e, \"TrainMSE:\", train_acc, \"TestMSE:\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw.export_scalars('scalar_dict.json')\n",
    "sw.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
